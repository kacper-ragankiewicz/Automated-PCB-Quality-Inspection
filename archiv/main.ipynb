{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Paths to the dataset\n",
    "train_data_dir = \"pcb_data/train\"  # Replace with your training folder path\n",
    "test_data_dir = \"pcb_data/test\"    # Replace with your test folder path\n",
    "\n",
    "# Image dimensions\n",
    "# Increased resolution for better feature extraction\n",
    "IMG_HEIGHT, IMG_WIDTH = 128, 128\n",
    "BATCH_SIZE = 64                   # Larger batch size for smoother gradients\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,          # Normalize pixel values\n",
    "    rotation_range=30,          # Random rotation\n",
    "    width_shift_range=0.3,      # Horizontal shift\n",
    "    height_shift_range=0.3,     # Vertical shift\n",
    "    shear_range=0.3,            # Shearing transformation\n",
    "    zoom_range=0.3,             # Random zoom\n",
    "    horizontal_flip=True,       # Flip images horizontally\n",
    "    fill_mode=\"nearest\"         # Fill in missing pixels\n",
    ")\n",
    "\n",
    "# Rescale only for validation/testing (no augmentation)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "# Create data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"binary\"  # Binary classification: \"good\" (1) and \"bad\" (0)\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "# Option 1: Use a Sequential CNN model\n",
    "model = Sequential([\n",
    "    # Convolutional layers\n",
    "    Conv2D(32, (3, 3), activation=\"relu\",\n",
    "           input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(256, (3, 3), activation=\"relu\"),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(32, (3, 3), activation=\"relu\", kernel_regularizer=l2(0.01)),\n",
    "    # Add pooling layer after the last convolutional layer\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Dropout(0.5),  # Add dropout here for regularization\n",
    "\n",
    "    # Flatten the output for the fully connected layers\n",
    "    Flatten(),\n",
    "\n",
    "    # Fully connected layers\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dropout(0.5),  # Prevent overfitting\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")  # Sigmoid for binary classification\n",
    "])\n",
    "\n",
    "# Option 2: Use MobileNetV2 (pretrained feature extractor)\n",
    "# Uncomment to use MobileNetV2\n",
    "# base_model = MobileNetV2(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), include_top=False, weights=\"imagenet\")\n",
    "# base_model.trainable = False  # Freeze the base model\n",
    "# model = Sequential([\n",
    "#     base_model,\n",
    "#     Flatten(),\n",
    "#     Dense(128, activation=\"relu\"),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(1, activation=\"sigmoid\")\n",
    "# ])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),  # Lower learning rate for stability\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Callbacks for training\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    \"best_model.h5\", monitor=\"val_loss\", save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=30,  # Increase epochs to allow more learning\n",
    "    validation_data=test_generator,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Plot training and validation accuracy/loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.title(\"Accuracy Over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.title(\"Loss Over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Define the folder to save the plots\n",
    "folder = \"accuracy\"\n",
    "os.makedirs(folder, exist_ok=True)  # Ensure the folder exists\n",
    "\n",
    "# Get the next available number for the filename\n",
    "existing_files = [f for f in os.listdir(folder) if f.startswith(\n",
    "    \"training_plot\") and f.endswith(\".png\")]\n",
    "next_number = 1 + max(\n",
    "    [int(f.split(\"_\")[2].split(\".\")[0])\n",
    "     for f in existing_files if f.split(\"_\")[2].split(\".\")[0].isdigit()],\n",
    "    default=0\n",
    ")\n",
    "\n",
    "# Save the plot with the next number\n",
    "filename = f\"training_plot_1_{next_number}.png\"\n",
    "plt.savefig(os.path.join(folder, filename))\n",
    "\n",
    "print(f\"Plot saved as {filename} in the '{folder}' folder.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Paths to the dataset\n",
    "train_data_dir = \"pcb_data/train\"  # Replace with your training folder path\n",
    "test_data_dir = \"pcb_data/test\"    # Replace with your test folder path\n",
    "\n",
    "# Image dimensions\n",
    "IMG_HEIGHT, IMG_WIDTH = 128, 128  # Image size for MobileNetV2\n",
    "BATCH_SIZE = 64                   # Larger batch size for smoother gradients\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,          # Normalize pixel values\n",
    "    rotation_range=40,          # Increased random rotation\n",
    "    width_shift_range=0.4,      # Increased horizontal shift\n",
    "    height_shift_range=0.4,     # Increased vertical shift\n",
    "    shear_range=0.4,            # Increased shearing transformation\n",
    "    zoom_range=0.4,             # Increased random zoom\n",
    "    horizontal_flip=True,       # Flip images horizontally\n",
    "    fill_mode=\"nearest\"         # Fill in missing pixels\n",
    ")\n",
    "\n",
    "# Rescale only for validation/testing (no augmentation)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "# Create data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"binary\"  # Binary classification: \"good\" (1) and \"bad\" (0)\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "# Use Pretrained MobileNetV2 as the feature extractor\n",
    "base_model = MobileNetV2(input_shape=(\n",
    "    IMG_HEIGHT, IMG_WIDTH, 3), include_top=False, weights=\"imagenet\")\n",
    "base_model.trainable = False  # Freeze the base model to use it as a feature extractor\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(128, activation=\"relu\"),  # Fully connected layer\n",
    "    # Reduced dropout to prevent over-regularization\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation=\"sigmoid\")  # Sigmoid for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model with a reduced learning rate\n",
    "model.compile(\n",
    "    # Extremely low learning rate for stability\n",
    "    optimizer=Adam(learning_rate=0.000001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Callbacks for training\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    \"best_model.h5\", monitor=\"val_loss\", save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=30,  # Increase epochs for better optimization\n",
    "    validation_data=test_generator,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Plot training and validation accuracy/loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.title(\"Accuracy Over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.title(\"Loss Over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Define the folder to save the plots\n",
    "folder = \"accuracy\"\n",
    "os.makedirs(folder, exist_ok=True)  # Ensure the folder exists\n",
    "\n",
    "# Get the next available number for the filename\n",
    "existing_files = [f for f in os.listdir(folder) if f.startswith(\n",
    "    \"training_plot\") and f.endswith(\".png\")]\n",
    "next_number = 1 + max(\n",
    "    [int(f.split(\"_\")[2].split(\".\")[0])\n",
    "     for f in existing_files if f.split(\"_\")[2].split(\".\")[0].isdigit()],\n",
    "    default=0\n",
    ")\n",
    "\n",
    "# Save the plot with the next number\n",
    "filename = f\"training_plot_2_{next_number}.png\"\n",
    "plt.savefig(os.path.join(folder, filename))\n",
    "\n",
    "print(f\"Plot saved as {filename} in the '{folder}' folder.\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
